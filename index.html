---
search_exclude: true
image: images/logo.png
---

<!doctype html>
<html lang="en-US">
  <head>
    <title>Reshinth Adithyan</title>
    <link rel="icon" href="images/logo.png" type = "image/x-icon">
    <link rel="stylesheet" type="text/css" href="style.css">
  </head>
  <body>
    <section id="wrapper">
      <h1>Reshinth Adithyan</h1>
      <ul>
        <li>Email: <a href="reshinth.adith@gmail.com" target="_blank">reshinth.adith@gmail.com</a></li>
        <li>Twitter: <a href="https://twitter.com/reshinth_" target="_blank">@reshinth_</a></li>
        <li>Github: <a href="https://github.com/reshinthadithyan" target="_blank">reshinthadithyan</a></li>
      </ul>
      <h2>About Me</h2>
        This is Reshinth Adithyan, you can call me <b>Reshinth</b>. I am a Researcher/Research Engineer working in the intersection of <b>Machine Learning for Code Representation Learning and currently Neural Program Synthesis</b>.<br>
        I have started to liking the process of writing parsers<b>(Not just for code, but also musical notations)</b>. I generally use Bison based parsing tools(Jison). Have started to write Parsers without Shift Reduce Conflict [Bad Joke]<br>
        As of 2022, I am focussed on Research of Neural Program Synthesis.</strong>
      <h2>Projects with Code Representation Learning</h2>
      <ul>
        <li>
          <strong><a href="https://github.com/CodedotAl/gpt-code-clippy" target="_blank">GPT-CODE-CLIPPY</a></strong>
          Was part of the core team in buiding the open source GitHub CoPilot during the huggingface flax/jax sprint.
        </li>
        <li><strong><a href="https://huggingface.co/spaces/reshinthadith/code-representation-learning" target="_blank"> Code Representation Learning</a></strong>is a collective of a Machine Learning on Code tasks, which are ready to showcase using HuggingFace Spaces.</li>
        <li><strong><a href="https://huggingface.co/reshinthadith/codet5-type-inference" target="_blank"> Type Inference using AutoRegressive Pre-Trained Transformers</a></strong> A research project on Finetuning the codeT5 Model to infer the type of identifier in a given code snippet.</li>
        <li><strong><a href="https://github.com/reshinthadithyan/flashfill-program-synthesis" target="_blank"> Flash Fill Program Synthesis</a></strong> Interpreted IO based program synthesis using Pre-Trained Transformers.</li>
        </ul>
      <h2>Have a look at my Research Proposal</h2>
      <ul>
        <a href="assets/files/intrinsic_eval_proposal.pdf" target="_blank">Intrinsic Evaluation for Models trained on Code</a> Examining how well models trained with CausalLM objective, knows the context of the code.
      </ul>
      <h2>Research Interests</h2>
      <ul>
        <li>Code Representation Learning.</li>
        <li>Parsers and Compilers.</li>
        <li>Geometric Representation Learning in DFG(s)/CFG(s)</li>
        <li>Naturalness in Code.</li>
        <li>Neural Program Synthesis</li>
        <li>Domain Specific Language Engineering</li>
        <li><strong><a href = "https://openprocessing.org/user/267866?view=sketches" target="_blank"> Creative Coding.</strong></li></a>
      </ul>
      <h2>Tools</h2>
      <ul>
        <li><strong>Languages :</strong> python, javascript, haskell, rust </li>
        <li><strong>Deep Learning Tools :</strong> pytorch, jax, jraph, tensorflow, keras, huggingface Transformers, Pytorch Geometric. </li>
        <li><strong>Parser Generators :</strong>jison, python lex-yacc, antlr</li>
      </ul>
      <h2>Posts</h2>
      <ul>
        {% for post in site.posts %}
          <li>
            <a href="{{ post.url }}">{{ post.title }}</a>
          </li>
        {% endfor %}
      </ul>
      <footer>
        <p>CSS style adapted from <a href="https://spolu.vercel.app/">Stanislas Polu's Webpage</a> </p>
        <p><a href="mailto:reshinth.adith@gmail.com">contact me @ here</a></p>
      </footer>